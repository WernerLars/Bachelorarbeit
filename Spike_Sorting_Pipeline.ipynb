{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spike Sorting Pipeline Notebook by Lars Werner (3066030)\n",
    "\n",
    "### In this notebook you can use SpikeForest2, SpikeInterface and my lists for Spike Sorting. For detailed information what the functions do, look into the SpikeSortingPipeline.py library.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Imports, which are also used in the SpikeSortingPipeline.py library.\n",
    "from spikeforest2_utils import AutoRecordingExtractor, AutoSortingExtractor\n",
    "import kachery as ka\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.toolkit as st\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface.comparison as sc\n",
    "import spikeinterface.widgets as sw\n",
    "import json\n",
    "\n",
    "#More useful imports\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#Use the Command below to print Graphs in Jupyter Notebook (Not recommended for many recordings)\n",
    "#%matplotlib notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For importing SpikeSortingPipeline library (not needed in this notebook, is used in Use_Case Notebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import SpikeSortingPipeline as ssp\n",
    "#help(ssp)\n",
    "#recording = ssp.createRecordingList()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to load your own Recording, you can use the template below or look in Use_Case Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fill in the following Data\n",
    "#file_path = \n",
    "#recording_name =\n",
    "#dtype=\n",
    "#offset=\n",
    "#mode=\n",
    "#channels=\n",
    "#sampling_frequency =\n",
    "\n",
    "### Then use numpy to load a memmap and you may reshape your data\n",
    "#data=np.memmap(file_path+recording_name, dtype=dtype, offset=offset, mode=mode)\n",
    "#data=data.reshape(channels,len(data)//channels)\n",
    "\n",
    "### Use the right Extractor for your Recording\n",
    "### you can look up installed Extractors with \n",
    "\n",
    "#se.installed_Recording_Extractors()?\n",
    "#se.installed_Sorting_Extractors()?\n",
    "\n",
    "#recording = se.NumpyRecordingExtractor(timeseries=data, sampling_frequency = sampling_frequency)\n",
    "\n",
    "### You can load a probe file\n",
    "#probe_name =\n",
    "#recording = recording.load_probe_file(file_path+probe_name)\n",
    "\n",
    "#recordings = createRecordingList()\n",
    "#recordings = addRecording(recordings, recording, \"\", \"Own Data\")\n",
    "#printRecordingData(recordings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions for loading, storing and printing SpikeForest Recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRecordingList():  \n",
    "    \n",
    "    return list()\n",
    "\n",
    "def addRecording(recording_list, recording, sorting_true, study_set_name):  \n",
    "\n",
    "    recording_name = \"recording_\"+str(len(recording_list))\n",
    "    recording_list.append([[recording_name,study_set_name], recording, sorting_true])\n",
    "    return recording_list\n",
    "\n",
    "def getSpikeForestStudySetList():\n",
    "    \n",
    "    return [\"PAIRED_BOYDEN\", \"PAIRED_CRCNS_HC1\", \"PAIRED_ENGLISH\", \"PAIRED_KAMPFF\", \"PAIRED_MEA64C_YGER\",\n",
    "            \"PAIRED_MONOTRODE\",\"HYBRID_JANELIA\", \"LONG_DRIFT\", \"LONG_STATIC\", \"SYNTH_BIONET\", \"SYNTH_MAGLAND\",\n",
    "            \"SYNTH_MEAREC_NEURONEXUS\",\"SYNTH_MEAREC_TETRODE\", \"SYNTH_MONOTRODE\", \"SYNTH_VISAPY\", \"MANUAL_FRANKLAB\"]\n",
    "\n",
    "def loadSpikeForestRecordings(recording_list, study_set_name, study_set_number, recording_intervall, download):  \n",
    "    \n",
    "    ka.set_config(fr='default_readonly')   \n",
    "    \n",
    "    json_file_name = \"spikeforest_recordings/recordings/\"+study_set_name+\"/\"+study_set_name+\".json\"\n",
    "    json_file = open(json_file_name)\n",
    "    data = json.load(json_file) \n",
    "    \n",
    "    study = data[\"studies\"][study_set_number]\n",
    "    recordings = study[\"recordings\"][recording_intervall[0]:recording_intervall[1]]\n",
    "    \n",
    "    for rec in recordings:\n",
    "        recording = AutoRecordingExtractor(rec['directory'], download=download)\n",
    "        sorting_true = AutoSortingExtractor(rec['firingsTrue'])    \n",
    "        recording_list = addRecording(recording_list, recording, sorting_true, rec[\"studyName\"]+\" : \"+rec[\"name\"])   \n",
    "        \n",
    "    return recording_list\n",
    "\n",
    "def printRecordingData(recordings):   \n",
    "    \n",
    "    for rec in recordings:  \n",
    "\n",
    "        print(rec[0][0], \":\", rec[0][1])\n",
    "        recording = rec[1]\n",
    "        channel_ids = recording.get_channel_ids()\n",
    "        fs = recording.get_sampling_frequency()\n",
    "        num_chan = recording.get_num_channels()\n",
    "\n",
    "        print('Channel ids:', channel_ids)\n",
    "        print('Sampling frequency:', fs)\n",
    "        print('Number of channels:', num_chan)\n",
    "    \n",
    "        sorting_true = rec[2]\n",
    "        if sorting_true != \"\":\n",
    "            unit_ids = sorting_true.get_unit_ids()\n",
    "            spike_train = sorting_true.get_unit_spike_train(unit_id=unit_ids[0])\n",
    "\n",
    "            print('Unit ids:', unit_ids)\n",
    "            print('Spike train of first unit:', spike_train, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Way to print all Study Set Names, which can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "study_set_list = getSpikeForestStudySetList()\n",
    "\n",
    "for study_set_name in study_set_list:\n",
    "    print(study_set_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Recording List, adding three SpikeForest Recordings to it and printing the list and some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recordings = createRecordingList()\n",
    "recordings = loadSpikeForestRecordings(recordings, study_set_list[1], 0, [0,1] , True)\n",
    "recordings = loadSpikeForestRecordings(recordings, study_set_list[10], 0, [0,1] , True)\n",
    "recordings = loadSpikeForestRecordings(recordings, study_set_list[15], 0, [0,1] , True)\n",
    "\n",
    "print(recordings, \"\\n\")\n",
    "printRecordingData(recordings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These two cells print a timeseries of the first stored recording in the list (channel 0 in range 0-0.01). They visualize before and after bandpass filtering a recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sw.plot_timeseries(recordings[0][1], channel_ids = [0] , trange=[0,0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = st.preprocessing.bandpass_filter(recordings[0][1])\n",
    "sw.plot_timeseries(test, channel_ids = [0] , trange=[0,0.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These Methods can print Timeseries, Geometry, Spectrum, Spectogram, Raster, Isi Distribution and Correlograms for a list of recordings. The printed Graphs are stored in the figures folder and also plotted in the notebook (see imports)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTimeseries(recordings):\n",
    "    \n",
    "    for recording in recordings:  \n",
    "        plot = sw.plot_timeseries(recording[1],  channel_ids = [0,1] , trange=[0,0.01])\n",
    "        plot.figure.suptitle(recording[0][0]+\" : \"+recording[0][1])\n",
    "        plot.figure.savefig(\"figures/\"+\"timeseries_\"+recording[0][0])\n",
    "        \n",
    "def printElectrodeGeometry(recordings):\n",
    "\n",
    "    for recording in recordings:\n",
    "        plot = sw.plot_electrode_geometry(recording[1])\n",
    "        plot.figure.suptitle(recording[0][0]+\" : \"+recording[0][1])\n",
    "        plot.figure.savefig(\"figures/\"+\"geometry_\"+recording[0][0])\n",
    "        \n",
    "def printSpectrum(recordings):\n",
    "    \n",
    "    for recording in recordings:\n",
    "        plot = sw.plot_spectrum(recording[1])\n",
    "        plot.figure.suptitle(recording[0][0]+\" : \"+recording[0][1])\n",
    "        plot.figure.savefig(\"figures/\"+\"spectrum_\"+recording[0][0])\n",
    "        \n",
    "def printSpectrogram(recordings):\n",
    "    \n",
    "    for recording in recordings:\n",
    "        plot = sw.plot_spectrogram(recording[1], channel=0, nfft=2048)\n",
    "        plot.figure.suptitle(recording[0][0]+\" : \"+recording[0][1])\n",
    "        plot.figure.savefig(\"figures/\"+\"spectrogram_\"+recording[0][0])\n",
    "        \n",
    "def printRasters(recordings):\n",
    "    \n",
    "    for recording in recordings:\n",
    "        plot = sw.plot_rasters(recording[2], sampling_frequency = recording[1].get_sampling_frequency())\n",
    "        plot.figure.suptitle(recording[0][0]+\" : \"+recording[0][1])\n",
    "        plot.figure.savefig(\"figures/\"+\"rasters_\"+recording[0][0])\n",
    "        \n",
    "def printIsiDistribution(recordings):\n",
    "    \n",
    "    for recording in recordings:\n",
    "        plot = sw.plot_isi_distribution(recording[2], bins=10, window=1, \n",
    "                                 sampling_frequency = recording[1].get_sampling_frequency())\n",
    "        plot.figure.suptitle(recording[0][0]+\" : \"+recording[0][1])\n",
    "        plot.figure.savefig(\"figures/\"+\"isi_distribution_\"+recording[0][0])\n",
    "        \n",
    "def printAutocorrelograms(recordings):\n",
    "\n",
    "    for recording in recordings:\n",
    "        plot = sw.plot_autocorrelograms(recording[2], bin_size=1, window=10, \n",
    "                                 sampling_frequency = recording[1].get_sampling_frequency())\n",
    "        plot.figure.suptitle(recording[0][0]+\" : \"+recording[0][1])\n",
    "        plot.figure.savefig(\"figures/\"+\"autocorrelograms_\"+recording[0][0])\n",
    "\n",
    "def printCrosscorrelograms(recordings):\n",
    "    \n",
    "    for recording in recordings:\n",
    "        plot = sw.plot_crosscorrelograms(recording[2], bin_size=0.1, window=5, \n",
    "                                  sampling_frequency = recording[1].get_sampling_frequency())\n",
    "        plot.figure.suptitle(recording[0])\n",
    "        plot.figure.savefig(\"figures/\"+\"crosscorrelograms_\"+recording[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the methods to print the graphs (stored in figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "printTimeseries(recordings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printElectrodeGeometry(recordings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printSpectrogram(recordings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printIsiDistribution(recordings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printAutocorrelograms(recordings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "printCrosscorrelograms(recordings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These methods are for running the Spike Sorting for a list of recordings with all installed Spike Sorters, creating a list that contains all recordings and their sorters with sorting results. You can also print the SorterList and some SorterData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runSpikeSorting(recordings,working_folder):\n",
    "    \n",
    "    recording_list = list()\n",
    "    for recording in recordings:\n",
    "        recording_list.append(recording[1])\n",
    "    \n",
    "    spike_sorting = ss.run_sorters(sorter_list = ss.installed_sorters(),\n",
    "                               recording_dict_or_list = recording_list,\n",
    "                               working_folder = working_folder)\n",
    "    \n",
    "    return spike_sorting\n",
    "\n",
    "def createSorterList(recordings,spike_sorting):\n",
    "    \n",
    "    sorter_list = list()\n",
    "    \n",
    "    for recording in recordings:\n",
    "        \n",
    "        sorters = list()\n",
    "        \n",
    "        for key in spike_sorting:\n",
    "            \n",
    "            if recording[0][0] == key[0]:\n",
    "                \n",
    "                sorters.append([key[1],spike_sorting[key]])\n",
    "            \n",
    "        sorter_list.append([recording,sorters])\n",
    "        \n",
    "    return sorter_list\n",
    "\n",
    "def printSorterList(sorter_list):\n",
    "    \n",
    "    for sorter in sorter_list:\n",
    "        print(sorter,\"\\n\")\n",
    "        \n",
    "def printSorterData(sorter_list):\n",
    "    \n",
    "    for entry in sorter_list:\n",
    "\n",
    "        print(\"-\"*50,\"\\n\")\n",
    "        print(entry[0][0],\"\\n\")\n",
    "        print(\"-\"*50,\"\\n\")\n",
    "        \n",
    "        for sorter in entry[1]:\n",
    "            \n",
    "            print(sorter[0],\"\\n\")\n",
    "            sorting = sorter[1]\n",
    "            recording = entry[0][1]\n",
    "    \n",
    "            snrs = st.validation.compute_snrs(sorting, recording)\n",
    "            isi_violations = st.validation.compute_isi_violations(sorting, \n",
    "                                                          duration_in_frames=recording.get_num_channels())\n",
    "            isolations = st.validation.compute_isolation_distances(sorting, recording)\n",
    "\n",
    "            print('SNR', snrs,\"\\n\")\n",
    "            print('ISI violation ratios', isi_violations,\"\\n\")\n",
    "            print('Isolation distances', isolations, \"\\n\"*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This prints your installed Sorters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ss.installed_sorters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runs all SpikeSorters for your recordings. Note that the working folder must not exist to run this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spike_sorting = runSpikeSorting(recordings,\"working_folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads your working folder where your sorting results are saved. You dont need to run SpikeSorting every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_sorting = ss.collect_sorting_outputs(\"working_folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the SorterList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(spike_sorting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates and prints the SorterList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorter_list = createSorterList(recordings,spike_sorting)\n",
    "printSorterList(sorter_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prints some SorterData for every Recording "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printSorterData(sorter_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These Cells let you print the Unit Waveforms and Unit Templates for the first recoring in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wf = st.postprocessing.get_unit_waveforms(recordings[0][1], recordings[0][2], ms_before=1, ms_after=2,\n",
    "                                          save_as_features=True, verbose=True)\n",
    "print(recordings[0][2].get_shared_unit_spike_feature_names())\n",
    "print(wf[0].shape)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(wf[0][:, 0, :].T, color='k', lw=0.3)\n",
    "#ax.plot(wf[1][:, 0, :].T, color='r', lw=0.3)\n",
    "#ax.plot(wf[2][:, 0, :].T, color='b', lw=0.3)\n",
    "#ax.plot(wf[3][:, 0, :].T, color='k', lw=0.3)\n",
    "fig.savefig(\"figures/waveform.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "templates = st.postprocessing.get_unit_templates(recordings[0][1], recordings[0][2], max_spikes_per_unit=200,\n",
    "                                                 save_as_property=True, verbose=True)\n",
    "print(recordings[0][2].get_shared_unit_property_names())\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(templates[0].T, color='b')\n",
    "#ax.plot(templates[1].T, color='r')\n",
    "#ax.plot(templates[2].T, color='g')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are functions to print Graphs of the Unit Waveforms, Amplitude Distribution, Amplitude Timeseries and PCA Features for a Sorterlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printUnitWaveforms(sorter_list):\n",
    "    \n",
    "    for rec in sorter_list:       \n",
    "        recording = rec[0][1]        \n",
    "        for sorter in rec[1]:\n",
    "            sorting = sorter[1] \n",
    "            plot = sw.plot_unit_waveforms(recording, sorting, max_spikes_per_unit=100)\n",
    "            plot.figure.suptitle(rec[0][0][0]+\" : \"+rec[0][0][1]+\" : \"+sorter[0])\n",
    "            plot.figure.savefig(\"figures/\"+\"unit_waveforms_\"+sorter[0]+\"_\"+rec[0][0][0])\n",
    "            \n",
    "            \n",
    "def printAmplitudeDistribution(sorter_list):\n",
    "\n",
    "    for rec in sorter_list:       \n",
    "        recording = rec[0][1]        \n",
    "        for sorter in rec[1]:\n",
    "            sorting = sorter[1]   \n",
    "            plot = sw.plot_amplitudes_distribution(recording, sorting, max_spikes_per_unit=300)\n",
    "            plot.figure.suptitle(rec[0][0][0]+\" : \"+rec[0][0][1]+\" : \"+sorter[0])\n",
    "            plot.figure.savefig(\"figures/\"+\"amplitude_distribution_\"+sorter[0]+\"_\"+rec[0][0][0])\n",
    "    \n",
    "def printAmplitudeTimeseries(sorter_list):\n",
    "\n",
    "    for rec in sorter_list:       \n",
    "        recording = rec[0][1]        \n",
    "        for sorter in rec[1]:\n",
    "            sorting = sorter[1]   \n",
    "            plot = sw.plot_amplitudes_timeseries(recording, sorting, max_spikes_per_unit=300)\n",
    "            plot.figure.suptitle(rec[0][0][0]+\" : \"+rec[0][0][1]+\" : \"+sorter[0])\n",
    "            plot.figure.savefig(\"figures/\"+\"amplitude_timeseries_\"+sorter[0]+\"_\"+rec[0][0][0])\n",
    "    \n",
    "def printPCAFeatures(sorter_list):\n",
    "    \n",
    "    for rec in sorter_list:       \n",
    "        recording = rec[0][1]        \n",
    "        for sorter in rec[1]:\n",
    "            sorting = sorter[1] \n",
    "            plot = sw.plot_pca_features(recording, sorting, colormap='rainbow', nproj=1, max_spikes_per_unit=100)\n",
    "            plot.figure.suptitle(rec[0][0][0]+\" : \"+rec[0][0][1]+\" : \"+sorter[0])\n",
    "            plot.figure.savefig(\"figures/\"+\"pca_features_\"+sorter[0]+\"_\"+rec[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the methods to print the graphs (stored in figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printUnitWaveforms(sorter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printAmplitudeDistribution(sorter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printAmplitudeTimeseries(sorter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printPCAFeatures([sorter_list[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to compare and print the performance of the results using ground truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareWithGroundTruth(sorter_list):\n",
    "    \n",
    "    for rec in sorter_list:       \n",
    "        sorting_true = rec[0][2]\n",
    "        if not sorting_true == \"\":\n",
    "            for sorter in rec[1]:\n",
    "                sorting = sorter[1]\n",
    "                comp = sc.compare_sorter_to_ground_truth(sorting_true,sorting)\n",
    "                w_comp = sw.plot_confusion_matrix(comp)\n",
    "                w_comp.figure.suptitle(\"GroundTruth_Confusion_Matrix : \"+rec[0][0][0]+\" : \"+rec[0][0][1]+\" : \"+sorter[0])\n",
    "                w_comp.figure.savefig(\"figures/\"+\"compare_groundtruth_confusionmatrix_\"+sorter[0]+\"_\"+rec[0][0][0])\n",
    "                w_agr = sw.plot_agreement_matrix(comp) \n",
    "                w_agr.figure.suptitle(\"GroundTruth_Agreement_Matrix : \"+rec[0][0][0]+\" : \"+rec[0][0][1]+\" : \"+sorter[0])\n",
    "                w_agr.figure.savefig(\"figures/\"+\"compare_groundtruth_agreementmatrix_\"+sorter[0]+\"_\"+rec[0][0][0])\n",
    "\n",
    "              \n",
    "def printPerformance(sorter_list):\n",
    "     \n",
    "    for rec in sorter_list:\n",
    "        recording = rec[0][1]\n",
    "        sorting_true = rec[0][2]\n",
    "        if not sorting_true == \"\":\n",
    "            for sorter in rec[1]:\n",
    "                sorting = sorter[1]\n",
    "                comp = sc.compare_sorter_to_ground_truth(sorting_true,sorting)               \n",
    "                w_perf_acc = sw.plot_sorting_performance(comp, property_name='snr', metric='accuracy')\n",
    "                w_perf_acc.figure.suptitle(\"Accurcay : \"+rec[0][0][0]+\" : \"+rec[0][0][1]+\" : \"+sorter[0])\n",
    "                w_perf_acc.figure.savefig(\"figures/\"+\"performance_accuracy_\"+sorter[0]+\"_\"+rec[0][0][0])               \n",
    "                w_perf_rec = sw.plot_sorting_performance(comp, property_name='snr', metric='recall')\n",
    "                w_perf_rec.figure.suptitle(\"Recall : \"+rec[0][0][0]+\" : \"+rec[0][0][1]+\" : \"+sorter[0])\n",
    "                w_perf_rec.figure.savefig(\"figures/\"+\"performance_recall_\"+sorter[0]+\"_\"+rec[0][0][0])              \n",
    "                w_perf_precision = sw.plot_sorting_performance(comp, property_name='snr', metric='precision')\n",
    "                w_perf_precision.figure.suptitle(\"Precision : \"+rec[0][0][0]+\" : \"+rec[0][0][1]+\" : \"+sorter[0]) \n",
    "                w_perf_precision.figure.savefig(\"figures/\"+\"performance_precision_\"+sorter[0]+\"_\"+rec[0][0][0])              \n",
    "                w_perf_false_dis_rate = sw.plot_sorting_performance(comp, property_name='snr', metric='false_discovery_rate')\n",
    "                w_perf_false_dis_rate.figure.suptitle(\"False Discovery Rate : \"+rec[0][0][0]+\" : \"+rec[0][0][1]+\" : \"+sorter[0]) \n",
    "                w_perf_false_dis_rate.figure.savefig(\"figures/\"+\"performance_false_discovery_rate_\"+sorter[0]+\"_\"+rec[0][0][0])            \n",
    "                w_perf_miss_rate = sw.plot_sorting_performance(comp, property_name='snr', metric='miss_rate')\n",
    "                w_perf_miss_rate.figure.suptitle(\"Miss Rate : \"+rec[0][0][0]+\" : \"+rec[0][0][1]+\" : \"+sorter[0]) \n",
    "                w_perf_miss_rate.figure.savefig(\"figures/\"+\"performance_miss_rate_\"+sorter[0]+\"_\"+rec[0][0][0])\n",
    "            \n",
    "            \n",
    "def compareSorters(sorter_entry):\n",
    "    \n",
    "    for sorter1 in sorter_entry[1]:\n",
    "        for sorter2 in sorter_entry[1]:\n",
    "            if sorter1[0] != sorter2[0]:\n",
    "                cmp = sc.compare_two_sorters(sorting1=sorter1[1], \n",
    "                                             sorting2=sorter2[1],\n",
    "                                             sorting1_name=sorter1[0],\n",
    "                                             sorting2_name=sorter2[0])\n",
    "                plot = sw.plot_agreement_matrix(cmp)\n",
    "                plot.figure.suptitle(sorter_entry[0][0][0] + \" : \" + sorter1[0] + \" - \" + sorter2[0])\n",
    "                plot.figure.savefig(\"figures/\"+\"compare_sorter_\"+sorter_entry[0][0][0]+\"_\"+sorter1[0]+\"_\"+sorter2[0])\n",
    "                print(sorter_entry[0][0][0] + \" : \" + sorter1[0] + \" - \" + sorter2[0] + \"\\n\")\n",
    "                print(cmp.match_event_count)\n",
    "                print(cmp.agreement_scores)\n",
    "                print(\"\\n\")\n",
    "\n",
    "def compareMultipleSorters(sorter_list):\n",
    "    \n",
    "    for rec in sorter_list:\n",
    "        sorters = list()\n",
    "        for sorter in rec[1]:\n",
    "            sorters.append(sorter[1])\n",
    "        multicomp = sc.compare_multiple_sorters(sorters)\n",
    "        w_multi = sw.plot_multicomp_graph(multicomp, edge_cmap='coolwarm', node_cmap='viridis', draw_labels=False,\n",
    "                                  colorbar=True)\n",
    "        w_multi.figure.suptitle(rec[0][0][0])\n",
    "        w_multi.figure.savefig(\"figures/\"+\"compare_multi_sorters_\"+rec[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the methods to print the graphs (stored in figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compareWithGroundTruth(sorter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printPerformance(sorter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compareSorters(sorter_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compareSorters(sorter_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareSorters(sorter_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareMultipleSorters(sorter_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This was the first try for implementing an Autoencoder for Spike Sorting. This does not work, but could be a starting point for new projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recording = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "autoencoder = Sequential(\n",
    "    [\n",
    "        Input(shape=(fs,)),\n",
    "        Dense(32,activation='relu'),\n",
    "        Dense(16,activation='relu'),\n",
    "        Dense(8,activation='relu'),\n",
    "        Dense(16,activation='relu'),\n",
    "        Dense(32,activation='relu'),\n",
    "        Dense(fs,activation='sigmoid')\n",
    "    ]\n",
    ")\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recording = st.preprocessing.bandpass_filter(recording, cache_to_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data of Recording\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recording_data = recording.get_traces(channel_ids=channel_ids,start_frame=0,end_frame=fs)\n",
    "\n",
    "print(recording_data)\n",
    "\n",
    "print(\"Number of Arrays :\", len(recording_data))\n",
    "print(\"Length of one Array :\", len(recording_data[0,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam',\n",
    "                    loss = 'mae',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "autoencoder.fit(recording_data,recording_data, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
